{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HealthCostRegression.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9TX15KOkPBV"
      },
      "source": [
        "*Note: You are currently reading this using Google Colaboratory which is a cloud-hosted version of Jupyter Notebook. This is a document containing both text cells for documentation and runnable code cells. If you are unfamiliar with Jupyter Notebook, watch this 3-minute introduction before starting this challenge: https://www.youtube.com/watch?v=inN8seMm7UI*\n",
        "\n",
        "---\n",
        "\n",
        "In this challenge, you will predict healthcare costs using a regression algorithm.\n",
        "\n",
        "You are given a dataset that contains information about different people including their healthcare costs. Use the data to predict healthcare costs based on new data.\n",
        "\n",
        "The first two cells of this notebook import libraries and the data.\n",
        "\n",
        "Make sure to convert categorical data to numbers. Use 80% of the data as the `train_dataset` and 20% of the data as the `test_dataset`.\n",
        "\n",
        "`pop` off the \"expenses\" column from these datasets to create new datasets called `train_labels` and `test_labels`. Use these labels when training your model.\n",
        "\n",
        "Create a model and train it with the `train_dataset`. Run the final cell in this notebook to check your model. The final cell will use the unseen `test_dataset` to check how well the model generalizes.\n",
        "\n",
        "To pass the challenge, `model.evaluate` must return a Mean Absolute Error of under 3500. This means it predicts health care costs correctly within $3500.\n",
        "\n",
        "The final cell will also predict expenses using the `test_dataset` and graph the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rRo8oNqZ-Rj"
      },
      "source": [
        "# Import libraries. You may or may not use all of these.\n",
        "#!pip install -q git+https://github.com/tensorflow/docs\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers.experimental import preprocessing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiX2FI4gZtTt"
      },
      "source": [
        "# Import data\n",
        "# dataset_path = keras.utils.get_file(\"insurance.csv\", \"https://cdn.freecodecamp.org/project-data/health-costs/insurance.csv\")\n",
        "!wget \"https://cdn.freecodecamp.org/project-data/health-costs/insurance.csv\"\n",
        "costs = pd.read_csv('insurance.csv')\n",
        "costs.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irnxakm7G8iv"
      },
      "source": [
        "# Pandas dataframe to tensorflow.data dataset.\n",
        "# Requires whole dataframe, with features to use and target.\n",
        "# https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers\n",
        "def df_to_ds(df, target, shuffle=True, batch=32):\n",
        "    features = df.copy()\n",
        "    labels = features.pop(target)\n",
        "\n",
        "    ds = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
        "\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(buffer_size=len(features))\n",
        "\n",
        "    ds = ds.batch(batch)\n",
        "    ds = ds.prefetch(batch)\n",
        "\n",
        "    return ds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vi3xCnncG87O"
      },
      "source": [
        "def get_normalization_layer(dataset, feature):\n",
        "    # Create a Normalization layer for our feature.\n",
        "    normalizer = preprocessing.Normalization()\n",
        "\n",
        "    # Prepare a Dataset that only yields our feature.\n",
        "    feature_ds = dataset.map(lambda x, y: x[feature])\n",
        "\n",
        "    # Learn the statistics of the data.\n",
        "    normalizer.adapt(feature_ds)\n",
        "\n",
        "    return normalizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUfHiOqiHFCF"
      },
      "source": [
        "def get_category_encoding_layer(dataset, feature, dtype, max_tokens=None):\n",
        "    # Create a StringLookup layer which will turn strings into integer\n",
        "    # indices.\n",
        "    if dtype == 'string':\n",
        "        index = preprocessing.StringLookup(max_tokens=max_tokens)\n",
        "    else:\n",
        "        index = preprocessing.IntegerLookup(max_values=max_tokens)\n",
        "\n",
        "    # Prepare a dataset that only yields our feature.\n",
        "    feature_ds = dataset.map(lambda x, y: x[feature])\n",
        "\n",
        "    # Learn the set of possible values and assign them a fixed integer index.\n",
        "    index.adapt(feature_ds)\n",
        "\n",
        "    # Create a discretization for our integer indices.\n",
        "    encoder = preprocessing.CategoryEncoding(max_tokens=index.vocab_size())\n",
        "\n",
        "    # Prepare a dataset that only yields our feature.\n",
        "    feature_ds = feature_ds.map(index)\n",
        "\n",
        "    # Learn the space of possible indices.\n",
        "    encoder.adapt(feature_ds)\n",
        "\n",
        "    # Apply one-hot encoding to our indices. The lambda function\n",
        "    # captures the layer so we can use them, or include them in the\n",
        "    # functional model later.\n",
        "    return lambda x: encoder(index(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcopvQh3X-kX"
      },
      "source": [
        "# Process costs.\n",
        "# Columns:\n",
        "# 'age':  integer (categorical?)\n",
        "# 'sex':  categorical\n",
        "# 'bmi':  float\n",
        "# 'children':  integer\n",
        "# 'smoker':  categorical\n",
        "# 'region':  categorical\n",
        "# 'expenses':  float; training target\n",
        "\n",
        "# Split 70%/20%/10% training/validation/testing.\n",
        "# pandas.DataFrame.sample():  Set random_state to reproduce partitioning.\n",
        "# https://stackoverflow.com/a/38251213/12968623\n",
        "\n",
        "# Data frames.\n",
        "train_d, val_d, test_d = \\\n",
        "    np.split(costs.sample(frac=1, random_state=42),\n",
        "                [int(0.70 * len(costs)), int(0.90 * len(costs))])\n",
        "\n",
        "# Features.\n",
        "train_f = train_d.copy()\n",
        "val_f = val_d.copy()\n",
        "test_f = test_d.copy()\n",
        "\n",
        "# Labels (targets).\n",
        "train_l = train_f.pop('expenses')\n",
        "val_l = val_f.pop('expenses')\n",
        "test_l = test_f.pop('expenses')\n",
        "\n",
        "train_ds = df_to_ds(train_d, 'expenses', shuffle=True, batch=256)\n",
        "val_ds = df_to_ds(val_d, 'expenses', shuffle=False, batch=256)\n",
        "test_ds = df_to_ds(test_d, 'expenses', shuffle=False, batch=256)\n",
        "\n",
        "# Build model.\n",
        "\n",
        "# Numeric features:  age, bmi, children.\n",
        "numerics = ['age', 'bmi', 'children']\n",
        "# numerics = ['bmi', 'children']\n",
        "# numerics = ['bmi']\n",
        "# Categorical features:  sex, smoker, region.\n",
        "categoricals = ['sex', 'smoker', 'region']\n",
        "# categoricals = ['sex', 'smoker']\n",
        "# categoricals = ['smoker']\n",
        "# Categorical features as integers:  age.  Maybe?\n",
        "# categorical_integers = ['age']\n",
        "categorical_integers = []\n",
        "\n",
        "inputs = []\n",
        "encodeds = []\n",
        "\n",
        "# Numeric features.\n",
        "for feature in numerics:\n",
        "    input = tf.keras.Input(shape=(1,), name=feature)\n",
        "    normalization = get_normalization_layer(train_ds, feature)\n",
        "    encoded = normalization(input)\n",
        "    inputs.append(input)\n",
        "    encodeds.append(encoded)\n",
        "\n",
        "# Categorical features encoded as integers.\n",
        "for feature in categorical_integers:\n",
        "    input = tf.keras.Input(shape=(1,), name=feature, dtype='int64')\n",
        "    encoding = get_category_encoding_layer(train_ds,\n",
        "                                            feature,\n",
        "                                            dtype='int64',\n",
        "                                            max_tokens=5)\n",
        "    encoded = encoding(input)\n",
        "    inputs.append(input)\n",
        "    encodeds.append(encoded)\n",
        "\n",
        "# Categorical features encoded as strings.\n",
        "for feature in categoricals:\n",
        "    input = tf.keras.Input(shape=(1,), name=feature, dtype='string')\n",
        "    encoding = get_category_encoding_layer(train_ds,\n",
        "                                            feature,\n",
        "                                            dtype='string',\n",
        "                                            max_tokens=5)\n",
        "    encoded = encoding(input)\n",
        "    inputs.append(input)\n",
        "    encodeds.append(encoded)\n",
        "\n",
        "encoded_layers = tf.keras.layers.concatenate(encodeds)\n",
        "x = tf.keras.layers.Dense(4096, activation='relu')(encoded_layers)\n",
        "x = tf.keras.layers.Dropout(0.6)(x)\n",
        "x = tf.keras.layers.Dense(2048, activation='relu')(x)\n",
        "x = tf.keras.layers.Dropout(0.4)(x)\n",
        "x = tf.keras.layers.Dense(1024, activation='relu')(x)\n",
        "x = tf.keras.layers.Dropout(0.4)(x)\n",
        "x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
        "x = tf.keras.layers.Dropout(0.4)(x)\n",
        "x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
        "x = tf.keras.layers.Dropout(0.4)(x)\n",
        "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
        "x = tf.keras.layers.Dropout(0.4)(x)\n",
        "x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
        "# x = tf.keras.layers.Dropout(0.4)(x)\n",
        "output = tf.keras.layers.Dense(1)(x)\n",
        "model = tf.keras.Model(inputs, output)\n",
        "model.compile(optimizer='adam',\n",
        "                loss=tf.keras.losses.MeanAbsoluteError(),\n",
        "                metrics=['mae', 'mse'])\n",
        "\n",
        "tf.keras.utils.plot_model(model=model,\n",
        "                            rankdir=\"LR\",\n",
        "                            dpi=72,\n",
        "                            show_shapes=True)\n",
        "\n",
        "epochs = 30\n",
        "history = model.fit(train_ds,\n",
        "                    validation_data=val_ds,\n",
        "                    epochs=epochs)\n",
        "\n",
        "test_dataset = test_f\n",
        "test_labels = test_l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xe7RXH3N3CWU"
      },
      "source": [
        "# RUN THIS CELL TO TEST YOUR MODEL. DO NOT MODIFY CONTENTS?\n",
        "# Test model by checking how well the model generalizes using the test set.\n",
        "# loss, mae, mse = model.evaluate(test_dataset, test_labels, verbose=2)\n",
        "loss, mae, mse = model.evaluate(test_ds, verbose=2)\n",
        "\n",
        "print(\"Testing set Mean Abs Error:  ${:9,.2f} expenses\".format(mae))\n",
        "\n",
        "if mae < 3500:\n",
        "  print(\"You passed the challenge. Great job!\")\n",
        "else:\n",
        "  print(\"The Mean Absolute Error must be less than $3,500. Keep trying.\")\n",
        "\n",
        "# Plot predictions.\n",
        "test_predictions = model.predict(test_ds).flatten()\n",
        "\n",
        "a = plt.axes(aspect='equal')\n",
        "plt.scatter(test_labels, test_predictions)\n",
        "plt.xlabel('True values (expenses)')\n",
        "plt.ylabel('Predictions (expenses)')\n",
        "lims = [0, 50000]\n",
        "plt.xlim(lims)\n",
        "plt.ylim(lims)\n",
        "_ = plt.plot(lims,lims)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}